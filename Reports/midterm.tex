%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Graph ML for IMTS - Midterm Report
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,9pt,twoside]{article}
\hyphenpenalty=8000
\textwidth=160mm
\textheight=250mm
\usepackage[top=0.5cm, bottom=0.5cm, inner=1cm, outer=1cm, includehead]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\raggedbottom
\usepackage{xurl}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[hidelinks, pdftex]{hyperref}
\urlstyle{same}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{subcaption}
\pagenumbering{arabic}
\setcounter{page}{1}
\usepackage[english]{babel}
\usepackage{float}

\begin{document}
\fancyhead[LE]{\thepage\ \ \ \ Roudbari, Gianantonio, Sanganeria, Reza}
\fancyhead[RO]{Graphs for Irregular Multivariate Time Series - Midterm Report\ \ \ \ \thepage}

\begin{center}
\LARGE
\textbf{Graphs for Irregular Multivariate Time Series}\\[8pt]
\Large
\textbf{Midterm Progress Report}\\[12pt]
\normalsize
\textbf{Asal Roudbari, Luca Gianantonio, Saahil Sanganeria, Nawal Reza}\\[4pt]
\textit{Graph Machine Learning - Fall 2025}\\[2pt]
\end{center}

\section{Project Overview}\label{s:overview}

Irregular Multivariate Time Series (IMTS) are ubiquitous in critical healthcare applications where observations are collected at non-uniform intervals with varying sampling rates across physiological variables \cite{zhang2024irregular}. Unlike regular time series, IMTS present unique challenges: intra-series irregularity from uneven sampling intervals, inter-series asynchrony from different variables sampled at distinct rates, and multi-scale temporal patterns spanning different granularities \cite{luohi}. Traditional approaches using imputation or Canonical Pre-Alignment distort temporal patterns and obscure meaningful missingness signals—clinically, the absence of a measurement can itself carry information.

Graph Neural Networks (GNNs) offer a promising solution by naturally representing measurements as nodes and modeling both temporal relationships (within variables) and inter-variable dependencies (between sensors). Recent research shows that combining intra-series and inter-series graph dynamics yields more accurate predictions \cite{hajisafi2024wavegnn}. Our project focuses on graph-based forecasting for irregular healthcare time series using the PhysioNet ICU dataset. We aim to: (1) implement and benchmark strong baseline methods (GRU-D, Latent ODE, SeFT, RainDrop, WaveGNN, T-PatchGNN, KAFNet), (2) implement Hi-Patch \cite{luohi} as our core model, and (3) propose novel improvements including learnable graph structures and generative modeling components.

\textbf{Team Contributions:} Asal Roudbari (Latent ODE, WaveGNN, Hi-Patch), Luca Gianantonio (SeFT, KAFNet, Learnable Graphs), Saahil Sanganeria (Data Preprocessing, RainDrop, Evaluation), Nawal Reza (GRU-D, T-PatchGNN, Generative VAE).

\textbf{Code Repository:} All code, preprocessing scripts, model implementations, and experimental results are available at \url{https://github.com/saahilsanganeriya/GraphML-imts}.

\section{Dataset and Problem Formulation}\label{s:data}

\subsection{Dataset Overview}
We use the PhysioNet/Computing in Cardiology Challenge 2012 dataset \cite{silva2012physionet}: 12,000 ICU patients with 48-hour records containing 5 static descriptors (Age, Gender, Height, ICUType, Weight) and 36 time-series variables (vitals, labs, interventions). Key characteristics include mean sequence length of 77.0±23.3 observations/patient, 85.7\% sparsity, mean inter-measurement interval of 0.64±0.49 hours, and variable coverage ranging from 7.4\% (TroponinI) to 99.0\% (HCT). The dataset encompasses four ICU types: Medical (38.9\%), Surgical (26.1\%), Cardiac Surgery Recovery (20.9\%), and Coronary Care (14.1\%), introducing heterogeneous monitoring protocols.

Our exploratory analysis reveals non-random missingness patterns reflecting clinical decision-making processes. High-coverage variables (HCT, BUN, Creatinine) represent routine panels, while specialized tests (TroponinI, Cholesterol) are ordered based on specific indications. Physiological variables show clinically plausible distributions with notable outliers reflecting illness severity (HR up to 300 bpm, Glucose 24-816 mg/dL). Within-patient measurement intervals exhibit high variability (CV=0.63±0.18), with continuous monitoring variables showing regular patterns while laboratory values exhibit sparser, event-driven timing.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{eda_results/irregularity_analysis.png}
\caption{Multi-dimensional analysis of time series irregularity showing measurement frequency, data sparsity, temporal variability, and variable coverage relationships.}
\label{fig:irregularity}
\end{figure}

\subsection{Data Preprocessing}
We implemented a comprehensive preprocessing pipeline transforming raw PhysioNet text files into PyTorch tensors. The pipeline downloads PhysioNet sets A, B, and C (~1GB, 12,000 patients), parses text files converting timestamps from HH:MM to normalized hours [0,1], and creates observation masks for missing values. We removed only NaN and infinite values while deliberately keeping outliers (HR=300, Glucose=800) as they represent real critical conditions rather than errors. We applied 60/20/20 train/validation/test split with seed=42 for reproducibility and z-score normalization using training set statistics only.

For graph construction, each measurement becomes a node characterized by normalized timestamp $t_i \in [0,1]$, variable index $v_i \in \{1,...,36\}$, and z-score normalized value $z_i \in \mathbb{R}$. We construct three edge types: (a) temporal edges connecting consecutive measurements of the same variable within $\Delta t_{\text{max}}=2$ hours to capture temporal evolution, (b) variable edges connecting different variables measured within $\epsilon=3$ minutes to capture physiological correlations during synchronized clinical assessments, and (c) self-loops for training stability. Graphs average ~150 nodes and ~500 edges per patient, maintaining clinical sparsity while enabling efficient GNN processing.

\subsection{Problem Formulation}
An irregular multivariate time series is defined as $\mathcal{S} = \{(t_j, z_j, v_j)\}_{j=1}^M$ where $t_j \in \mathbb{R}^+$ is the timestamp, $z_j \in \mathbb{R}$ is the observed value, $v_j \in \{1,...,V\}$ is the variable index, $M$ is total observations, and $V=36$ temporal variables. Given historical IMTS $\mathcal{S}_{\text{hist}} = \{(t_j, z_j, v_j) | t_j \leq t_{\text{split}}\}$ and forecast queries $\mathcal{Q} = \{q_k = (t_k^f, v_k^f) | t_k^f > t_{\text{split}}\}$, we learn $f: (\mathcal{S}_{\text{hist}}, \mathcal{Q}) \rightarrow \hat{\mathbf{Z}}$ predicting values $\hat{z}_k$ for each query.

\textbf{Task Specification:} We use the first 24 hours (timestamps 0 to 0.5) as input and forecast values in the 24-30 hour window (timestamps 0.5 to 0.625). This is a sequence prediction task predicting values at each observed (timestamp, variable) pair in the forecast window. Evaluation uses only observed values, respecting masks for missing data.

\section{Baseline Model Implementations}\label{s:baselines}

\subsection{RainDrop}
RainDrop \cite{zhang2021graph} models IMTS by learning latent sensor dependency graphs through neural message passing. Unlike sequential approaches, it constructs sensor graphs where nodes represent variables and directed edges denote learned dependencies. When observation $x_{i,u}^t$ is recorded, RainDrop (1) embeds it via $\bm{h}_{i,u}^t = \sigma(x_{i,u}^t \bm{R}_u)$, (2) propagates messages via graph attention with learned edge weights, (3) aggregates using temporal self-attention, and (4) produces predictions. This naturally handles irregularity by generating embeddings for unobserved sensors through message passing from active sensors.

We adapted the official implementation from classification to forecasting with minimal modifications: adding \texttt{forecasting\_mode}, modifying output head to predict 36 variables × 6 timesteps, and reshaping output to (batch, 36, 6). Implementation is device-agnostic supporting CUDA, MPS, and CPU. We trained for 50 epochs with batch size 64, learning rate $10^{-4}$, Adam optimizer, $d_{\text{model}}=72$, 4 attention heads, 2 transformer layers, dropout 0.3, and gradient clipping (max norm 1.0). Training on NVIDIA A100 took ~8.5 minutes.

RainDrop achieved validation RMSE 0.561, MAE 0.365, R² 0.519, and test RMSE 0.596, MAE 0.368, R² 0.494. The RMSE/MAE ratio of 1.54-1.62 indicates reasonable outlier robustness. Figure~\ref{fig:raindrop} shows steady convergence with training loss decreasing from 0.719 to 0.371 and validation RMSE from 0.756 to 0.561, along with validation and test predictions. Test set performance closely matches validation, indicating strong generalization. Visual inspection reveals good calibration overall with tendency to underpredict extreme values, common with MSE loss. Variables with higher measurement frequency (HR, BP) showed better performance than sparse laboratory values.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth]{raindrop_results/loss_curves.png}
\caption{Training curves}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth]{raindrop_results/predictions_vs_actual.png}
\caption{Validation predictions}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\linewidth}
\centering
\includegraphics[width=\linewidth]{raindrop_results/test_results/predictions_vs_actual.png}
\caption{Test predictions}
\end{subfigure}
\caption{RainDrop: (a) monotonic loss decrease indicating stable convergence, (b) validation predictions cluster along diagonal, (c) test predictions match validation closely, demonstrating good generalization.}
\label{fig:raindrop}
\end{figure}

\subsection{GRU-D}
GRU-D \cite{che2018grud} extends standard GRU with decay mechanisms to exploit "informative missingness." The custom cell takes augmented input including observed value, missingness mask, and time interval $\Delta$ since last observation. Trainable input decay gradually shifts missing values from last observation toward learned feature mean as $\Delta$ increases. Hidden decay prevents long missing periods from unduly influencing model memory.

Implementation uses 2-layer Multi-Layer GRUD with hidden size 128. Input tensor structure: Batch × 3 × Features × Sequence Length (Data, Mask, Delta). For deeper layers, mask and delta are cyclically expanded to match hidden state dimensions. We trained for 10 epochs with Adam ($\text{lr}=0.001$), MSE loss, batch size 64.

GRU-D achieved validation RMSE 0.6844, MAE 0.4558, R² 0.4401, and test RMSE 0.7322, MAE 0.4560, R² 0.4099, explaining ~41\% of variance. Figure~\ref{fig:gru_d} shows consistent convergence with validation R² increasing from 0.32 to 0.44. Scatter plots reveal dense clustering around perfect prediction for typical values (0.4-0.6 normalized range) but systematic bias toward mean for extreme values, indicating limited generalization to distribution tails. Main challenge was preparing data for multi-layer architecture, expanding mask/delta tensors to match deeper hidden state dimensions.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{nawal_results/gru-d/train_val_loss_curves.png}
\caption{Training curves}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{nawal_results/gru-d/pred_vs_actual.png}
\caption{Predictions vs. actual}
\end{subfigure}
\caption{GRU-D: (a) consistent convergence with training loss slightly higher than validation, (b) strong performance on typical values with regression to mean for extremes.}
\label{fig:gru_d}
\end{figure}

\subsection{SeFT}
SeFT \cite{horn2020seft} treats IMTS as unordered sets of observations, leveraging permutation-invariant set functions. Architecture consists of: (1) sinusoidal time embeddings capturing multi-scale temporal patterns, (2) observation encoder (MLP) transforming $(t,v,z)$ triplets to 128-D embeddings, (3) multi-head self-attention set encoder (4 heads, 3 layers), and (4) query-based decoder predicting future values via cross-attention.

Implementation from scratch with $d_{\text{model}}=128$, 4 heads, 3 layers, dropout 0.3 (705,729 parameters). Device-agnostic for CUDA/MPS/CPU. Trained 36 epochs, batch size 64, learning rate $3 \times 10^{-4}$, AdamW, gradient clipping (max norm 1.0). Main challenge was handling empty forecast windows from early discharge/death patients; added validation to skip patients with zero forecast observations, reducing training data by ~2\%.

SeFT achieved validation RMSE 0.8596, MAE 0.6265, R² 0.2610, explaining 26\% of variance. RMSE/MAE ratio of 1.37 suggests relative outlier robustness. Figure~\ref{fig:seft} shows gradual improvement plateauing after epoch 20, suggesting representational capacity limits, along with predictions clustering around 0.5-0.6 normalized range. This conservative mean prediction strategy is common in high-sparsity settings (85.7\% missing). Low R² reflects SeFT's limitation in capturing inter-variable dependencies by treating observations as unordered sets, missing physiological correlations.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{luca_results/seft_training_curve.png}
\caption{Training curves}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{luca_results/seft_prediction_vs_actual.png}
\caption{Predictions vs. actual}
\end{subfigure}
\caption{SeFT: (a) gradual improvement plateauing after epoch 20, (b) predictions cluster around 0.5-0.6 range, indicating conservative mean prediction strategy under high data sparsity.}
\label{fig:seft}
\end{figure}

\subsection{Latent ODE}
Latent ODE \cite{rubanova2019latentode} combines VAEs with neural ODEs for continuous-time dynamics modeling. Components include: (1) ODE-RNN encoder processing historical observations in reverse chronological order, (2) variational layer learning probabilistic latent space via reparameterization, and (3) ODE decoder evolving latent state forward and projecting to predictions. Trained with Adam ($\text{lr}=10^{-3}$), StepLR scheduler (step 10, decay 0.9), batch size 1 (due to variable-length sequences), 50 epochs, combined reconstruction (MSE) and KL divergence loss ($\beta=0.01$).

\textbf{Challenges:} Training is extremely slow due to batch size 1 requirement and expensive ODE solver calls. Results pending at submission time.

\subsection{WaveGNN}
WaveGNN combines Transformer-based temporal encoding with dynamic GNNs for IMTS forecasting. Unlike static graph approaches, it learns adaptive adjacency matrices capturing short-term correlations (via attention) and long-term dependencies (via learned embeddings). Integration of Time2Vec temporal encoding and learnable decay handles irregular sampling. Trained with AdamW ($\text{lr}=5 \times 10^{-4}$, weight decay $10^{-4}$), ReduceLROnPlateau scheduler, batch size 32, 50 epochs, masked MSE loss, gradient clipping (max norm 1.0), dropout 0.3.

WaveGNN achieved validation RMSE 0.72, MAE 0.508, R² 0.366, and test RMSE 0.75, MAE 0.54, R² 0.335. Figure~\ref{fig:wavegnn} shows training/validation convergence and per-feature analysis revealing varying performance across variables, with better results for regularly sampled vitals (HR, MAP, RespRate) versus sparse laboratory values (TroponinI, Cholesterol), highlighting the model's sensitivity to measurement frequency.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{Asal-results/Picture1.png}
\caption{Training curves}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{Asal-results/wavegnn_per_feature_predictions.png}
\caption{Per-feature predictions}
\end{subfigure}
\caption{WaveGNN: (a) training/validation convergence, (b) per-feature analysis showing varying performance with regularly sampled vitals achieving tighter clustering along diagonal compared to sparse laboratory measurements.}
\label{fig:wavegnn}
\end{figure}

\subsection{T-PatchGNN}
T-PatchGNN divides input into fixed-length patches to learn localized temporal patterns. Within each patch, Temporal-Temporal Convolutional Network (TTCN) captures fine-grained dynamics. Transformer Encoder with learned positional encodings models long-range dependencies across patches. Adaptive GNN module learns time-adaptive graph structures allowing variable relationships to evolve dynamically. MLP decoder projects aggregated representation to forecast horizon.

Implementation transforms raw sequences to patched format: (Batch, M=8, $L_{\text{in}}=8$, N=41) where M is patches, $L_{\text{in}}$ is patch length, N is features. Architecture: hidden dimension 64, 2 encoder layers, Adam ($\text{lr}=0.001$), MSE loss, batch size 32, 10 epochs. Main challenge was implementing sequence-to-patch transformation handling padding/truncation and structuring multi-dimensional timestamp/mask inputs.

T-PatchGNN achieved validation RMSE 0.6630, MAE 0.4424, R² 0.4513, and test RMSE 0.6656, MAE 0.4406, R² 0.4418. Figure~\ref{fig:t_patch} shows training instability with fluctuations in epochs 3 and 8, particularly visible in R² varying between 0.424-0.456. This suggests current learning rate/batch size configuration needs tuning. Despite instability, model demonstrates strong capacity (R²=0.44) with potential for improvement through hyperparameter optimization.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{nawal_results/t-patch/train_val_loss_curves.png}
\caption{T-PatchGNN training curves}
\label{fig:t_patch}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
\centering
\includegraphics[width=\linewidth]{luca_results/kafnet_training_curve.png}
\caption{KAFNet training curves}
\label{fig:kafnet}
\end{subfigure}
\caption{Training dynamics showing T-PatchGNN instability and KAFNet smooth convergence.}
\end{figure}

\subsection{KAFNet}
KAFNet combines temporal kernel aggregation with frequency-domain attention. (1) Kernel Aggregation uses RBF kernels weighting historical observations by temporal distance: $w_{ij} = \exp(-0.5 (|t_i - t_j| / \sigma)^2)$ with learnable bandwidth $\sigma$ per layer. (2) Frequency Attention applies FFT, performs multi-head attention in spectral domain, and applies inverse FFT to capture periodic patterns (e.g., circadian rhythms). Architecture processes observations through learned variable embeddings, RBF-weighted aggregation, frequency attention, and query-based MLP decoder.

Implementation from scratch: $d_{\text{model}}=128$, 4 heads, 3 layers, learnable log-scale bandwidth, 32 query points (924,581 parameters). Trained 36 epochs, batch size 64, AdamW ($\text{lr}=3 \times 10^{-4}$), gradient clipping (max norm 1.0), ~1.8 min/epoch on Apple Silicon MPS. Main challenge was initializing RBF bandwidth; fixed $\sigma=0.1$ performed poorly (RMSE>0.85), but learnable log-bandwidth improved validation RMSE by 18\% (0.85→0.72). Also handled empty batches and implemented dynamic FFT padding to nearest power of 2 with attention masking.

KAFNet achieved validation RMSE 0.7187, MAE 0.4821, R² 0.4835, representing 16\% improvement over SeFT (RMSE 0.8596) and explaining 85\% more variance (vs. SeFT R² 0.26). Training RMSE 0.7731, R² 0.3983 shows slight overfitting but modest gap suggests good generalization from kernel smoothing. Figure~\ref{fig:kafnet} shows smooth convergence. Predictions exhibit better spread along diagonal versus SeFT's clustered predictions, indicating improved patient-specific forecasting. Variables with higher frequency (HR, BP) showed R²~0.55 versus sparse labs (Troponin, Cholesterol) R²~0.25, highlighting dependence on temporal density for effective kernel smoothing.

\section{Results and Discussion}\label{s:results}

Tables~\ref{tab:val_results} and~\ref{tab:test_results} summarize performance across all implemented baselines. RainDrop achieves the best validation performance (RMSE 0.561, R² 0.519) and test performance (RMSE 0.596, R² 0.494), demonstrating the effectiveness of learning sensor dependency graphs through neural message passing. The model's ability to propagate information between variables proves crucial for handling the 85.7\% data sparsity in our dataset.

\begin{table}[h]
\centering
\caption{Validation set performance (all metrics on z-score normalized values)}
\begin{tabular}{lccccc}
\hline
\textbf{Model} & \textbf{RMSE} & \textbf{MAE} & \textbf{R²} & \textbf{RMSE/MAE} & \textbf{Loss (MSE)} \\
\hline
RainDrop & \textbf{0.561} & \textbf{0.365} & \textbf{0.519} & 1.538 & 0.291 \\
GRU-D & 0.684 & 0.456 & 0.440 & 1.502 & 0.468 \\
T-PatchGNN & 0.663 & 0.442 & 0.451 & 1.499 & 0.440 \\
KAFNet & 0.719 & 0.482 & 0.484 & 1.491 & 0.517 \\
WaveGNN & 0.720 & 0.508 & 0.366 & 1.420 & 0.490 \\
SeFT & 0.860 & 0.627 & 0.261 & 1.372 & 0.739 \\
Latent ODE & - & - & - & - & - \\
\hline
\end{tabular}
\label{tab:val_results}
\end{table}

\begin{table}[h]
\centering
\caption{Test set performance (all metrics on z-score normalized values)}
\begin{tabular}{lccccc}
\hline
\textbf{Model} & \textbf{RMSE} & \textbf{MAE} & \textbf{R²} & \textbf{RMSE/MAE} & \textbf{Loss (MSE)} \\
\hline
RainDrop & \textbf{0.596} & \textbf{0.368} & \textbf{0.494} & 1.621 & 0.337 \\
GRU-D & 0.732 & 0.456 & 0.410 & 1.606 & 0.536 \\
T-PatchGNN & 0.666 & 0.441 & 0.442 & 1.511 & 0.443 \\
WaveGNN & 0.750 & 0.540 & 0.335 & 1.400 & 0.643 \\
KAFNet & - & - & - & - & - \\
SeFT & - & - & - & - & - \\
Latent ODE & - & - & - & - & - \\
\hline
\end{tabular}
\label{tab:test_results}
\end{table}

\textbf{Key Findings:} GRU-D and T-PatchGNN show comparable strong performance (val R² 0.44-0.45), demonstrating that both RNN-based decay mechanisms and patch-based GNN approaches can effectively model irregular clinical time series. KAFNet substantially outperforms SeFT (16\% RMSE improvement), validating that explicit temporal modeling via kernel smoothing and frequency attention outperforms permutation-invariant set encoders for our task. WaveGNN's moderate performance (val R² 0.366) suggests that dynamic graph learning alone may not be sufficient without proper regularization for extremely sparse data.

\textbf{Common Patterns:} (1) All models exhibit RMSE/MAE ratios of 1.4-1.7, indicating reasonable outlier robustness despite extreme clinical values; (2) predictions tend to underestimate extreme values due to MSE loss encouraging conservative estimates; (3) performance varies significantly by variable type, with regularly sampled vitals (HR, BP) showing higher R² than sparse laboratory values (TroponinI, Cholesterol); (4) training instability observed in T-PatchGNN and initial WaveGNN experiments highlights the need for careful hyperparameter tuning; (5) test performance generally matches validation closely (e.g., RainDrop 0.561→0.596 RMSE), indicating good generalization without significant overfitting.

\textbf{Computational Challenges:} Latent ODE faces significant challenges with batch size 1 and expensive ODE solver calls, making full dataset training infeasible within midterm timeline. This motivates our action plan to reduce dataset size for computationally intensive models.

\section{Action Plan}\label{s:plan}

\textbf{Completed Work:} We have successfully implemented and benchmarked six baseline models (RainDrop, GRU-D, SeFT, WaveGNN, T-PatchGNN, KAFNet), developed comprehensive data preprocessing pipeline with graph construction, conducted extensive EDA, and established evaluation framework with consistent train/val/test splits.

\textbf{Final Report Deliverables:}
\begin{enumerate}
\item \textbf{Hi-Patch Implementation:} Implement hierarchical patch GNN with intra-patch layers (fully-connected graphs, GAT, multi-time attention) and inter-patch layers (multi-scale P, 2P, 4P connections). Expected to outperform baselines by capturing both fine-grained local patterns and coarse-grained global dependencies.

\item \textbf{Novel Contributions:} (a) Learnable graph structures replacing fixed patch adjacency with attention-based edge weights computed from node features, enabling patient/time-specific connectivity; (b) Generative VAE component adding reconstruction loss alongside forecasting to regularize embeddings and enable synthetic trajectory generation.

\item \textbf{Hyperparameter Optimization:} Systematic tuning for all models (learning rate, batch size, hidden dimensions, number of layers, dropout rates) to maximize performance and ensure fair comparison.

\item \textbf{Extended Analysis:} Per-variable performance breakdown, temporal pattern visualization, attention/graph structure visualization for interpretability, ablation studies on graph construction strategies.
\end{enumerate}

\textbf{Improvement Strategies:}
\begin{enumerate}
\item \textbf{Feature Selection:} Drop variables with >60\% patient-level missingness (TroponinI 7.4\%, Cholesterol 9.6\%, etc.) to reduce noise and improve learning efficiency. Retain high-coverage vitals and labs (HCT, BUN, Creatinine, HR, MAP, RespRate).

\item \textbf{Dataset Size Reduction:} For computationally intensive models (Latent ODE, WaveGNN with full attention), use stratified subset (e.g., 30-50\% of data) maintaining ICU type distribution. This enables faster iteration and hyperparameter exploration.

\item \textbf{Training Stabilization:} Implement learning rate warmup, gradient accumulation for small batch sizes, early stopping with patience, and mixed precision training for memory efficiency.

\item \textbf{Task Formulation Variants:} Explore (a) shorter forecast horizons (12h, 6h instead of 24-30h), (b) next-timestamp prediction per variable instead of full sequence, (c) patch-level aggregated targets for stability.
\end{enumerate}

\textbf{Backup Plans:} If Hi-Patch training proves unstable, use top-k sparsification for attention, correlation-based fixed graphs, or simpler GCN/GraphSAGE layers. If generative VAE degrades forecast performance, use curriculum learning (forecast-only → reconstruction) with KL annealing, or replace with simpler self-supervised auxiliary tasks (patch contrast, masked prediction).

\section{Conclusion}\label{s:conclusion}

We have made substantial progress implementing and evaluating seven baseline models for irregular multivariate time series forecasting on the PhysioNet ICU dataset. RainDrop achieves the strongest performance (test RMSE 0.596, R² 0.494) through learned sensor dependency graphs, demonstrating the power of graph-based approaches for sparse, irregular clinical data. GRU-D, T-PatchGNN, and KAFNet show competitive results (R² 0.41-0.48), validating diverse methodological approaches from decay mechanisms to patch-based GNNs to kernel aggregation.

Our comprehensive data preprocessing pipeline, consistent evaluation framework, and preliminary results establish a solid foundation for the final phase. Key insights include the importance of explicit temporal modeling (KAFNet outperforms SeFT by 16\%), the challenge of extreme value prediction under MSE loss, and the computational constraints of continuous-time models (Latent ODE). Moving forward, we will implement Hi-Patch and novel contributions (learnable graphs, generative VAE), perform systematic hyperparameter optimization, apply targeted improvements (feature selection, dataset reduction), and conduct deeper analysis to deliver a rigorous final report demonstrating the power of graph machine learning for critical healthcare applications.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}

